---
title: 'MantaID': A machine-learning-based tool to automate the identification of biological database IDs
author: Zeng Zhengpeng
date: 2022/5/1
output: rmdformats::material
editor_options: 
  markdown: 
    wrap: 72
---

## Import

biomaRt, ggplot2, caret, data.table, dplyr, keras, magrittr, mlr3, purrr, reshape2, scutr, stringr, tibble, tidyr, tidyselect, paradox.

```{R}
#Note: Run the following code for installing biomaRt, mlr3, mlr3tuning packages of specific version.
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager",repos = "http://cran.us.r-project.org")
if (!requireNamespace("remotes", quietly = TRUE))
    install.packages("remotes",repos = "http://cran.us.r-project.org")
library(remotes)
install_version("mlr3learners","0.5.1",force = T,upgrade ="never")
install_version("mlr3tuning","0.13.1",force = T,upgrade ="never")
install_version("mlr3","0.13.4",force = T,upgrade ="never")
library(BiocManager)
BiocManager::install("biomaRt", version = "3.16",force = TRUE)
```

## Installation

```{R}
if (!requireNamespace("devtools", quietly = TRUE))
    install.packages("devtools")
install_bitbucket("Molaison/MantaID")
```

### Data Retrieving

biomaRt provides an interface to R and the BioMart software suite databases (e.g. Ensembl, Uniprot, HapMap), allowing direct access to information in the databases via R.

Set the BioMart dataset to be connected to via biomaRt. Here we choose to use the human genome dataset and asia mirror (depending on the region).

First, use 'mi_get_ID_attr' function to get the attributes of the dataset associated with the ID. Then utilize 'flt_attri' function further by looking at the dataset to select the dataset of interest. Finally, the incoming attributes are automatically retrieved by the 'mi_get_ID' function, which then compiles the findings into a large table.

```{R}
#Select the Ensemble dataset of eligible human genes in the BioMart database.
attributes = mi_get_ID_attr(biomart = "genes", dataset = "hsapiens_gene_ensembl", mirror = "asia")
#Select the attributes in lines 1, 2, 6, and 7.
flt_attri = attributes %>% dplyr::slice(1,2,6,7)
#Automatically retrieve incoming attributes, and process the results into a long table.
data_ID = mi_get_ID(flt_attri,biomart = "genes", dataset = "hsapiens_gene_ensembl", mirror = "asia")
data_ID
```

### Data Processing

Sometimes the data obtained is an ID mapping table, with each row corresponding to an ID entity, and each column corresponds to a different database. So, do the training you need to reorganize the table and remove invalid values. Use the 'mi_clean_data' function to do this. For example:

```{R}
#Create a 4x3 tibble data frame.
data <- tibble::tibble(
	"ensembl_gene_id" = c("ENSG00000001626","ENSG00000002549","ENSG00000002586","ENSG00000002745"),
	'ensembl_exon_id' = c("ENSE00002398851","ENSE00002398851","ENSE00002398851","ENSE00002398851"),
	'refseq_peptide' = c("NP_001303256","-","NP_001382772","NP_001340728")
)
#Re-arrange the tibble data box according to the ID and remove the rows with "-" in the ID.
data_ID_clean = mi_clean_data(data,placeholder="-")
```

The features of ID, in contrast to other data, are the placements of the constituent characters. Since it is impossible to train on only one column of "ID," it is split into a single character vector to extract the features, and the vector is filled with "*" up to the maximum ID to guarantee consistent data dimension.

```{R}
#Get max length of ID data.
pad_len = mi_get_padlen(data_ID)
#Cut the string of ID column character by character and divide it into multiple columns.
data_splt = mi_split_col(data_ID,cores = NULL,pad_len = pad_len)
#View the internal structure of the data.
str(data_splt)
```

It is necessary to convert the existing feature columns to factor types because they are currently strings and cannot be used for training. Because of characters appear in different feature columns in a different order, the levels parameter must be set in order to standardize the factor levels. This can then be used directly as a numeric type, but for compatibility reasons it will also be converted into a numeric type.

```{R}
#Convert data to numeric, and for the ID column convert with fixed levels.
data_fct = mi_to_numer(data_splt,levels = c("*", 0:9, letters, LETTERS, "_", ".", "-", " ", "/", "\\", ":"))
```

### Data Balancing

To prevent the trained model from losing its ability to distinguish between databases with small numbers of IDs, it is necessary to balance the data. On the one hand, the smote method is used to oversample databases with small numbers of IDs to increase the data density, and on the other hand, databases with large numbers of IDs are undersampled through random sampling to reduce the number.

The data set produced by balancing cannot be used as the test set any longer, so the ratio proportion of the original data set is divided as the test set, and this portion of the data is removed from the balanced data set, while the remaining portion is used as the training set. It should be noted, however, that the parallel parameter is only supported by Mac.

```{R}
#Balance data.
data_blcd = mi_balance_data(data_fct,ratio = 0.3,parallel = F)
```

### Models Training

Due to the large size of the dataset, the model training time is too long, so only a certain number of samples are taken for training; where the training set and the dataset are divided by calling the partition function, which exists as an index of the original data; three models are used for benchmark training, namely decision tree, random forest and plain Bayes, and resampling is performed using the five-fold crossover method; benchmark() The training was performed, and the training and test sets were evaluated separately after training (costs&ce); accepts four parameters, all of which have default parameters except data; data is the incoming data, where the target column (i.e. the column where the ID database name is located) must have the column name "class", and all columns are of type factor.

The first thing that needs to be done is the initial selection of a machine learning classification model, where multiple models are trained and the most suitable ones are selected; The `row_num` parameter determines the number of data items to be used, if the data is large then a portion of the data will be extracted for testing, otherwise, all of the data will be used for testing. This step requires the use of the original dataset rather than the balanced dataset.

```{r echo=FALSE, message=FALSE, hide=TRUE}
#Repeatedly sampling 4000 rows of data, benchmarking for different learners, evaluating the training and test sets separately after training, and outputting a list of benchmarking results and scores for the test set.
result <- mi_run_bmr(data_fct, row_num = 4000)
#The first column of the list shows the benchmark results for the test set.
benchmark <- result[1]
#The second column of the list is a list of the scores of the test set, converting it into a table.
score <- result[2] %>% as.data.table() %T>% print()
```

The results were used to determine the choice of models for decision trees, random forests, and extreme gradient boosting.

```{R}
#Select the elements of the first column of the balanced data set, remove the class column, replace the missing values in the elements with 0, and use it as the training set.
train = data_blcd[[1]] %>% mutate(across(-class,.fns = ~tidyr::replace_na(.x,0))) %>% dplyr::slice(sample(nrow(data_blcd[[1]]), 2000), preserve = TRUE) 
#Select test set.
test = data_blcd[[2]]
#Tune the decision tree model by hyperband.
inst_rp <- mi_tune_rp(train, test)
#Train the decision tree model.  
result_rp <- mi_train_rp(train, test, measure = msr("classif.acc"), instance = inst_rp[[1]])
#Tune the random forest model by hyperband.
inst_rg <- mi_tune_rg(train, test)
#Train the random forest model.  
result_rg <- mi_train_rg(train, test, measure = msr("classif.acc"), instance = inst_rg[[1]])
#Tune the xgboost model by hyperband.
inst_xgb <- mi_tune_xgb(train, test)
#Train the xgboost model.
result_xgb <- mi_train_xgb(train, test, measure = msr("classif.acc"), instance = inst_xgb[[1]])
```

In addition to several classical machine learning algorithms, a BP neural network is used for classification.

This is achieved by calling `tensorflow` via the `keras` package, so `tensorflow` needs to be installed first. 

1. To install python and tensorflow dependencies, please follow the instructions provided by Tensorflow via [TensorFlow installation](https://www.tensorflow.org/install/pip?hl=zh-cn#system-install)

2. Use tensorflow in R. Please replace "/path/to/python.exe" with your python path.

```{R}
install.packages("reticulate")
library(reticulate)
library(tensorflow)
#path_to_python <- use_python(python = "/path/to/python.exe")
```

The meaning of parameters are (1) `train`, the training set; (2) `test`, the test set; (3) `path2save`, the path of the trained model, the default is NULL, not saved; (4) `batch_size`, the size of the training batch, the larger the training period, the shorter the training period, but the number of periods to achieve the same accuracy increases (5) `epochs`, the number of training periods, all samples are trained once; (6) `validation_split`, the proportion of data sets in the training set that are used to divide into validation sets.

```{R message=FALSE, include=FALSE}
#Train the neural network model.
result_BP <- mi_train_BP(train, test, path2save = NULL, batch_size = 128, epochs = 64, validation_split = 0.3)
```

### Models Explaining

`mi_get_confusion`, converts the results of model training into an obfuscation matrix; the results of the model training function are used directly as input; `ifnet`, a logical value, TRUE for a neural network model; `mi_plot_heatmap`, plots the heatmap for the confusion matrix; `name`, the model name, the suffix when the file is stored; `filepath`, the path when the model is stored.

```{R}
#Obfuscation matrix about the decision tree is obtained.
matri_rp <- mi_get_confusion(result_rp)
#Obfuscation matrix about random forest is obtained.
matri_rg <- mi_get_confusion(result_rg)
#Obfuscation matrix about xgboost is obtained.
matri_xgb <- mi_get_confusion(result_xgb)
#Obfuscation matrix about the neural network is obtained.
matri_BP <- mi_get_confusion(result_BP,ifnet = T)
#Confusion matrix heat map on decision tree.
mi_plot_heatmap(matri_rp, name="rp",filepath = "Graph/")
#Confusion matrix heat map on random forests.
mi_plot_heatmap(matri_rg, name="rg",filepath = "Graph/")
#Confusion matrix heat map on xgboost.
mi_plot_heatmap(matri_xgb, name="xgb",filepath = "Graph/")
#Confusion matrix heat map on neural network.
mi_plot_heatmap(matri_BP, name="BP",filepath = "Graph/")
```

```{r}
data("mi_data_rawID")
MantaID:::mi_unify_mod(mi_data_rawID, "ID",result_rg,result_rp,result_xgb,result_BP,c_value = 0.75, pad_len = pad_len)
```
