---
title: MantaID\:a machine-learning based tool to automate the identification of biological database IDs
author: Zeng Zhengpeng
date: 2022/5/1
output: rmdformats::material
editor_options: 
  markdown: 
    wrap: 72
---

```{r global-options, include=FALSE}
knitr::knit_hooks$set(time_it = local({
  now <- NULL
  function(before, options) {
    if (before) {
      # record the current time before each chunk
      now <<- Sys.time()
    } else {
      # calculate the time difference after a chunk
      res <- difftime(Sys.time(), now)
      # return a character string to show the time
      paste("Time for this code chunk to run:", res)
    }
  }
}))
knitr::opts_chunk$set(time_it = TRUE)
```

#### 包安装:

```{r}
packages <- c("biomartr", "biomaRt", "tidyverse", "magrittr", "data.table", "magrittr", "mlr3verse", "skimr", "roxygen2", "tidyverse", "parallel", "scutr", "reshape2", "RColorBrewer", "caret", "tensorflow", "keras", "reticulate", "skimr", "tfdatasets")
for (pac in packages) {
  tryCatch({
  	if(!require(pac)){install.packages(pac,repos = "https://cloud.r-project.org")}},warning = function(){
    tryCatch({
      if(!require(pac)){BiocManager::install(pac,update	=F)}
    },warning = function(){
      tryCatch({
        sprintf("Please verify whether package %s exists,if not then type 0 to pass.")
        if(!require(pac)){githubinstall::githubinstall(pac,ask = F)}
      },error = function(e)e)
    },error = function(e)e)
  },error = function(e)e)
}
if(!require(IDentifyEngine)){
	devtools::install_bitbucket("Molaison/IDentifyengine")
}

```

#### 数据获取:

biomaRt 向 R 和 BioMart software suite
的数据库（例如`Ensembl`,`Uniprot`,`HapMap`）提供了一个接口,可以通过R直接获取数据库中的信息。

通过`biomart`、`dataset`
设置要连接的`BioMart`数据集,这里选择使用人类基因组数据集,Mirror选择：`asia mirror`(根据地区而定);使用`mt_get_ID_attr`获取与ID相关的数据集的属性;

接着通过查看数据集对属性做进一步筛选,选取感兴趣数据集;
`mt_get_ID`函数自动检索传入的属性,并将结果处理成长表格。

```{r}

attributes = mt_get_ID_attr(biomart = "genes", dataset = "hsapiens_gene_ensembl", mirror = "asia")
flt_attri = attributes %>% slice(1,3,5,7)
data_ID = mt_get_ID(flt_attri,biomart = "genes", dataset = "hsapiens_gene_ensembl", mirror = "asia")
```

#### 数据处理

有时得到的数据是一个ID映射表，每行对应一个ID
entity,每列对应不同的数据库，这时要进行训练就需要重组表格,同时删去无效值;使用`mt_clean_data`函数来
完成。例如:

```{r}
data <- tibble::tibble(
	"ensembl_gene_id" = c("ENSG00000001626","ENSG00000002549","ENSG00000002586","ENSG00000002745"),
	'ensembl_exon_id' = c("ENSE00002398851","ENSE00002398851","ENSE00002398851","ENSE00002398851"),
	'refseq_peptide' = c("NP_001303256","-","NP_001382772","NP_001340728")
)
data_ID = mt_clean_data(data,placeholder="-")
```

与其他数据不同，ID的特征是组成的字符的位置,仅"ID"一列无法进行训练，故将其分割成单字符向量以获取特征，同时将向量用"\*"补长到最大ID的长度，保证数据维度一致。

```{r}
pad_len = mt_get_padlen(data_ID)
data_splt = mt_split_col(data_ID,cores = NULL,pad_len = pad_len)
str(data_splt)
```

当前特征列为字符串类型，尚无法用于训练,故需要将其转化为因子类型。同时由于不同特征列字符出现顺序不同，需要统一因子水平，故需要设置levels参数。接着即可直接当作数值型使用，但考虑到兼容问题,还会将其转化为数值型。

```{r}
data_fct = to_factor(data_splt,levels = c("*", 0:9, letters, LETTERS, "_", ".", "-", " ", "/", "\\", ":"))
```

### 数据平衡:

根据数据库的大小与重要性不同，检索到的ID数量差距会惊人的大,故需要对数据进行平衡，否则训练得到的模型丧失对ID数量小的数据库的分辨能力;一方面使用smote方法对ID数量少的数据库进行过采样，增加数据密度，另一方面通过随机采样对ID数量多的数据库进行欠采样，减少数量;通过平衡得到的数据集无法再作为测试集，故从原来数据集中划分出`ratio`比例的数据作为测试集，并将这部分数据从平衡后数据集中去除，剩余部分作为训练集;但是需要注意的是，`parallel`参数仅对Mac可用。

```{r}
data_blcd = balance_data(data_ID,ratio = 0.3,parallel = F)
```

#### 模型训练(决策树+随机森林+xgboost):

由于数据集过大,模型训练时间过长,故只抽取一定数量样本进行训练;其中训练集与数据集调用`partition`函数进行划分,以原数据索引形式存在；采用三种模型进行基准训练,分别为决策树、随机森林和朴素贝叶斯,使用五折交叉法进行重抽样；`benchmark()`执行训练,训练完毕后分别对训练集和测试集进行评估(costs&ce);
接受四个参数,除data外均有默认参数;`data`传入的数据,其中target列(也即ID数据库名所在列)列名必须为`"class"`,所有列的类型均为factor;

首先需要进行的是机器学习分类模型的初步选择,对多种模型进行训练，挑选出最合适的几种模型;
`row_num`参数决定了要使用的数据条数，若数据较大则抽取一部分进行测试，否则全部用于测试。这一步需要用原始数据集而非平衡后数据集;

```{r echo=FALSE, message=FALSE, hide=TRUE}
result <- mt_run_bmr(data_fct, row_num = 4000)
benchmark <- result[1]
score <- result[2] %>% as.data.table() %T>% print()
```

通过结果决定选用的模型为决策树，随机森林，以及xgboost。

```{r}
train = data_blcd[[1]]
test = data_blcd[[2]]
#决策树
result_rg <- mt_train_rg(train, test, measure = msr("classif.acc"))
#随机森林
result_rp <- mt_train_rp(train, test, measure = msr("classif.acc"))
#xgboost
result_xgboost <- mt_train_xgb(train, test, measure = msr("classif.acc"))
```

#### 神经网络模型:

除经典的几种机器学习算法外，还采用了BP神经网络进行分类。

通过`keras`包调用`tensorflow`实现，故需要先安装`tensorflow`。

```{r}
tensorflow::install_tensorflow()
```

参数的意义分别为(1)train,训练集;(2)test,测试集;(3)path2save,存储训练后模型的路径,默认为NULL,不保存;(4)batch_size,训练时的批次大小,越大训练单期时间越短,但达到相同精度的期数增大;(5)epochs,训练的期数,所有样本训练一次为一期;(6)validation_split,训练集中被用于划分为验证集的数据集的比例;

```{R}
result_net <- BPNN(train, test, path2save = NULL, batch_size = 128, epochs = 64, validation_split = 0.3)
```

### 混淆矩阵:

`cnfs_matri`
函数将模型训练得到的结果转化为混淆矩阵;直接以模型训练的函数得到的结果作为输入;`ifnet`参数为logical值,为TRUE时表示为神经网络模型;
`mt_plot_heatmap`则为混淆矩阵绘制热图;name,模型名,文件存储时后缀;filepath,模型存储时路径。

```{r}


matri_rg <- mt_get_confusion(result_rg)
matri_rp <- mt_get_confusion(result_rp)
matri_xgb <- mt_get_confusion(result_xgb)
matri_net <- mt_get_confusion(result_net,ifnet = T)

mt_plot_heatmap(matri_rg, name="rg",filepath = "Graph/")
mt_plot_heatmap(matri_rp, name="rp",filepath = "Graph/")
mt_plot_heatmap(matri_xgb, name="xgb",filepath = "Graph/")
mt_plot_heatmap(matri_net, name="xgb",filepath = "Graph/")
```
